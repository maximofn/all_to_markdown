# All to Markdown

Sistema completo para crawlear sitios de documentaci√≥n y convertir todas las p√°ginas a formato Markdown usando IA y [MarkItDown](https://github.com/microsoft/markitdown).

## üéØ ¬øQu√© hace este proyecto?

Este proyecto consta de **tres componentes principales** que trabajan juntos:

1. **üï∑Ô∏è Crawler Inteligente** (`crawl_documentation.py`): Usa GPT-5.1 para navegar autom√°ticamente por documentaci√≥n siguiendo enlaces de "siguiente p√°gina"
2. **üìÑ Conversor de URLs** (`url_to_markdown.py`): Convierte cualquier URL a Markdown (p√°ginas web, PDFs, videos de YouTube, etc.)
3. **üé¨ Orquestador Principal** (`main.py`): Coordina todo el proceso de crawling y conversi√≥n

## üìã Requisitos

- Python 3.10 o superior
- Clave de API de OpenAI (para el crawler)
- MarkItDown instalado con todas las dependencias

## üöÄ Instalaci√≥n

```bash
# Crear entorno virtual con uv
uv venv --python=3.12 .venv
source .venv/bin/activate

# Clonar e instalar MarkItDown
git clone git@github.com:microsoft/markitdown.git
uv pip install -e 'markitdown/packages/markitdown[all]'

# Instalar dependencias adicionales
uv pip install langchain-openai python-dotenv requests
```

### Configuraci√≥n de variables de entorno

Crea un archivo `.env` en la ra√≠z del proyecto:

```bash
OPENAI_API_KEY=tu_clave_api_aqui
```

## üíª Uso

### Opci√≥n 1: Script Principal (Recomendado)

Usa `main.py` para crawlear y convertir documentaci√≥n completa:

```bash
# Uso b√°sico
python main.py "https://www.gradio.app/main/guides/quickstart"

# Con directorio de salida personalizado
python main.py "https://docs.python.org/3/tutorial/" -o python_docs

# Con archivo de URLs personalizado
python main.py "https://example.com/docs" -o docs -f urls.txt

# Ver todas las opciones
python main.py --help
```

**¬øQu√© hace el script principal?**

1. üîç Crawlea la documentaci√≥n desde la URL base (usando IA)
2. üíæ Guarda la lista de URLs encontradas en un archivo
3. üìÑ Lee las URLs del archivo
4. üîÑ Convierte cada p√°gina a Markdown
5. üíæ Guarda todos los archivos en un directorio organizado

### Opci√≥n 2: Crawler de Documentaci√≥n

Usa `crawl_documentation.py` para solo obtener las URLs:

```bash
python crawl_documentation.py
```

**Configuraci√≥n del crawler** (edita las constantes en el archivo):

```python
# Model configuration
MODEL_NAME = "gpt-5.1"  # Modelo a usar
REASONING_EFFORT = "high"  # "low", "medium", o "high"

# Crawler configuration
MAX_PAGES = 500  # M√°ximo de p√°ginas a crawlear
DELAY_BETWEEN_PAGES = 2  # Segundos entre peticiones
```

El crawler:
- ü§ñ Usa GPT-5.1 para identificar enlaces de "siguiente p√°gina"
- üîó Maneja URLs relativas y absolutas autom√°ticamente
- üîÑ Evita loops detectando URLs ya visitadas
- üíæ Guarda todas las URLs en `documentation_urls.txt`
- ‚è±Ô∏è Incluye delays para ser respetuoso con los servidores

### Opci√≥n 3: Conversor Individual

Usa `url_to_markdown.py` para convertir URLs individuales:

```bash
# Convertir una p√°gina web
python url_to_markdown.py "https://www.python.org" output.md

# Convertir un video de YouTube (con transcripci√≥n)
python url_to_markdown.py "https://www.youtube.com/watch?v=VIDEO_ID" video.md

# Convertir un PDF
python url_to_markdown.py "https://example.com/document.pdf" document.md
```

### Como m√≥dulos Python

Todos los scripts pueden importarse y usarse en tus propios programas:

```python
# Usar el crawler
from crawl_documentation import crawl_documentation

urls = crawl_documentation("https://docs.example.com/start")
print(f"Found {len(urls)} pages")

# Usar el conversor
from url_to_markdown import convert_url_to_markdown

success = convert_url_to_markdown(
    url="https://www.python.org",
    output_path="output.md"
)

# Usar el orquestador
from main import process_documentation

process_documentation(
    base_url="https://docs.example.com",
    output_dir="my_docs",
    urls_file="my_urls.txt"
)
```

## üì¶ Tipos de contenido soportados

El conversor puede procesar:

- ‚úÖ P√°ginas web HTML
- ‚úÖ Videos de YouTube (t√≠tulo, descripci√≥n, transcripci√≥n)
- ‚úÖ Archivos PDF
- ‚úÖ Documentos de Office (Word, Excel, PowerPoint)
- ‚úÖ Im√°genes (con metadatos EXIF y OCR)
- ‚úÖ Archivos de audio (con transcripci√≥n)
- ‚úÖ P√°ginas de Wikipedia
- ‚úÖ Feeds RSS
- ‚úÖ Archivos CSV, JSON, XML
- ‚úÖ Archivos ZIP (itera sobre el contenido)
- ‚úÖ Y m√°s...

## üîß Estructura del proyecto

```
all_to_markdown/
‚îÇ
‚îú‚îÄ‚îÄ crawl_documentation.py       # Crawler inteligente con LLM
‚îÇ   ‚îú‚îÄ‚îÄ download_html()          # Descarga HTML de URLs
‚îÇ   ‚îú‚îÄ‚îÄ extract_next_link()      # Extrae siguiente enlace con IA
‚îÇ   ‚îî‚îÄ‚îÄ crawl_documentation()    # Funci√≥n principal de crawling
‚îÇ
‚îú‚îÄ‚îÄ url_to_markdown.py           # Conversor de URLs a Markdown
‚îÇ   ‚îú‚îÄ‚îÄ convert_url_to_markdown() # Convierte una URL
‚îÇ   ‚îî‚îÄ‚îÄ main()                   # CLI para uso standalone
‚îÇ
‚îú‚îÄ‚îÄ main.py                      # Orquestador principal
‚îÇ   ‚îú‚îÄ‚îÄ sanitize_filename()      # Genera nombres de archivo seguros
‚îÇ   ‚îú‚îÄ‚îÄ process_documentation()  # Proceso completo
‚îÇ   ‚îî‚îÄ‚îÄ main()                   # CLI del orquestador
‚îÇ
‚îú‚îÄ‚îÄ .env                         # Variables de entorno (API keys)
‚îú‚îÄ‚îÄ pyproject.toml              # Configuraci√≥n del proyecto
‚îî‚îÄ‚îÄ README.md                    # Esta documentaci√≥n
```

## üìñ Caracter√≠sticas principales

### üï∑Ô∏è Crawler Inteligente
- ü§ñ Usa GPT-5.1 con razonamiento de alta calidad
- üéØ Identifica autom√°ticamente enlaces de navegaci√≥n
- üîó Normaliza URLs (maneja rutas relativas, duplicados, etc.)
- üõ°Ô∏è Protecci√≥n contra loops infinitos
- ‚è±Ô∏è Rate limiting configurable
- üíæ Guarda progreso autom√°ticamente

### üìÑ Conversor Vers√°til
- ‚ú® Soporta m√∫ltiples tipos de contenido
- üìÅ Crea directorios autom√°ticamente
- ‚úÖ Validaci√≥n de URLs
- üìä Informaci√≥n detallada de conversi√≥n
- ‚ö†Ô∏è Manejo robusto de errores

### üé¨ Orquestador Completo
- üîÑ Proceso end-to-end automatizado
- üìä Progreso detallado con estad√≠sticas
- üè∑Ô∏è Nombres de archivo sanitizados
- üìÅ Organizaci√≥n autom√°tica de salida
- ‚å®Ô∏è Interrumpible con Ctrl+C

## üéì Ejemplos de uso avanzado

### Workflow completo paso a paso

```bash
# 1. Primero, crawlea la documentaci√≥n
python crawl_documentation.py

# 2. (Opcional) Edita documentation_urls.txt para filtrar URLs

# 3. Convierte todas las p√°ginas
python main.py "https://docs.example.com" --file documentation_urls.txt
```

### Procesar m√∫ltiples documentaciones

```python
from main import process_documentation

documentations = [
    ("https://docs.python.org/3/tutorial/", "python_docs"),
    ("https://docs.djangoproject.com/en/stable/", "django_docs"),
    ("https://flask.palletsprojects.com/", "flask_docs"),
]

for base_url, output_dir in documentations:
    print(f"\nProcessing {base_url}...")
    process_documentation(base_url, output_dir)
```

### Personalizar el crawler

```python
from crawl_documentation import crawl_documentation

# Crawlear con l√≠mite personalizado
urls = crawl_documentation(
    start_url="https://docs.example.com/intro",
    max_pages=100  # Solo las primeras 100 p√°ginas
)

# Guardar en tu propio formato
with open("my_urls.json", "w") as f:
    import json
    json.dump({"urls": urls}, f, indent=2)
```

## üîç Archivos generados

Despu√©s de ejecutar el proceso completo, encontrar√°s:

```
all_to_markdown/
‚îú‚îÄ‚îÄ documentation_urls.txt        # Lista de URLs crawleadas
‚îî‚îÄ‚îÄ markdown_output/              # Directorio con archivos MD
    ‚îú‚îÄ‚îÄ guides_quickstart.md
    ‚îú‚îÄ‚îÄ guides_interface.md
    ‚îú‚îÄ‚îÄ guides_blocks.md
    ‚îî‚îÄ‚îÄ ...
```

## ‚öôÔ∏è Configuraci√≥n avanzada

### Variables configurables en `crawl_documentation.py`

| Variable | Descripci√≥n | Valor por defecto |
|----------|-------------|-------------------|
| `MODEL_NAME` | Modelo de OpenAI a usar | `"gpt-5.1"` |
| `REASONING_EFFORT` | Nivel de razonamiento | `"high"` |
| `MAX_PAGES` | L√≠mite de p√°ginas | `500` |
| `DELAY_BETWEEN_PAGES` | Delay entre requests (seg) | `2` |

### Cambiar la URL base en el crawler

Edita la funci√≥n `main()` en `crawl_documentation.py`:

```python
def main():
    # Cambia esta URL por tu documentaci√≥n
    start_url = "https://tu-documentacion.com/inicio"
    
    all_urls = crawl_documentation(start_url)
    # ...
```

## üêõ Soluci√≥n de problemas

### Error: "OPENAI_API_KEY not found"

**Soluci√≥n**: Crea un archivo `.env` con tu clave de API:

```bash
echo "OPENAI_API_KEY=tu_clave_aqui" > .env
```

### El crawler no encuentra el siguiente enlace

**Posibles causas**:
- La documentaci√≥n no tiene enlaces de navegaci√≥n claros
- El HTML es muy complejo o din√°mico
- Intenta aumentar `REASONING_EFFORT` a `"high"`

### Error al convertir URLs

**Soluci√≥n**: Algunas URLs pueden requerir dependencias adicionales:

```bash
# Para videos de YouTube
uv pip install youtube-transcript-api

# Para PDFs
uv pip install pdfplumber

# Para im√°genes con OCR
uv pip install pytesseract
```

## üìù Archivos del proyecto

| Archivo | Descripci√≥n |
|---------|-------------|
| `crawl_documentation.py` | Crawler inteligente con LLM |
| `url_to_markdown.py` | Conversor de URLs a Markdown |
| `main.py` | Script principal orquestador |
| `.env` | Variables de entorno (no incluido) |
| `pyproject.toml` | Configuraci√≥n del proyecto Python |
| `README.md` | Esta documentaci√≥n |

## ü§ù Cr√©ditos

Este proyecto utiliza:
- [MarkItDown](https://github.com/microsoft/markitdown) de Microsoft para conversi√≥n a Markdown
- [LangChain](https://github.com/langchain-ai/langchain) para integraci√≥n con LLMs
- [OpenAI API](https://platform.openai.com/) para el razonamiento del crawler

## üìÑ Licencia

MIT License

